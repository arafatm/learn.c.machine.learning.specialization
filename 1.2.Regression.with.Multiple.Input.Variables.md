# C1_W2: Regression with Multiple Input Variables

This week, you'll extend linear regression to handle multiple input features. You'll also learn some methods for improving your model's training and performance, such as _vectorization_, _feature scaling_, _feature engineering_ and _polynomial regression_.  At the end of the week, you'll get to practice implementing linear regression in code.

## C1_W2_M1 Multiple Linear Regression

### C1_W2_M1_1 Multiple features

![](/img/1.2.1.1.multiple.features.png)
-$\vec{x}^{(i)}$= __vector__ of 4 parameters for$i^{th}$row
  =$[1416 3 2 40] $

![](/img/1.2.1.1.model.png)
- In this example, house price increase by (multiply 1k)
  - `0.1` per square foot
  - `4` per bedroom
  - `10` per floor
  - `-2` per year old
  - add `80` base price

![](/img/1.2.1.1.multiple.linear.regression.png)
- We can simplify the model 
- From linear algebra, this is a __row vector__ as opposed to _column vector_
- this is __multiple linear regression__
  - __Not__ _multivariate regression_

#### Quiz

In the training set below (see slide: C1_W2_M1_1 Multiple features), what is$x_{1}^{(4)} $?

<details><summary>Ans</summary>852</details>

### C1_W2_M1_2 Vectorization part 1

Learning to write __vectorized code__ allows you to take advantage of modern
numberical linear algebra libraries, as well as maybe GPU hardware.

![](/img/1.2.1.2.vectorization.png)
- Vector can be represented in Python as `np.array([1.0, 2.5, -3.3])` 
- if `n` is large, this code (on left) is inefficient
- for loop is more concise, but still not efficient
- `np.dot(w,x) + b` is most efficient using __vectorization__
- Vectorization has 2 benefits: _concise and efficient_
- `np.dot` can use parallel hardware

### C1_W2_M1_3 Vectorization part 2

How does vectorized algorithm works...

![](/img/1.2.1.3.vectorization.png) 
- Without vectorization, we run calculations linearly
- `np.dot` works in multiple steps:
  - get values of the vectors `w, x`
  - In parallel run `w[i] * x[i]`

![](/img/i1.2.1.3.gradient.descent.png)

### C1_W2_Lab01: Python Numpy Vectorization

- [Coursera](https://www.coursera.org/learn/machine-learning/ungradedLab/zadmO/optional-lab-python-numpy-and-vectorization/lab#?path=%2Fnotebooks%2FC1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb)
- [Local](/code/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb)
  -$a \cdot b$returns a scalar
  - e.g.$[1, 2, 3, 4] \cdot [-1, 4, 3, 2] = 24 $

### C1_W2_M1_4 Gradient descent for multiple linear regression

![](/img/1.2.1.4.gradient.descent.png)

![](/img/1.2.1.4.algorithm.png)
- `w & x` are now __vectors__
- have to update all the parameters simultaneously for$w_{1} .. w_{n}$as well as$b $

![](/img/1.2.1.4.normal.equation.png)
- __Normal Equation__
 
### C1_W2_Lab02: Muliple linear regression

- [Optional Lab: Multiple linear regression | Coursera](https://www.coursera.org/learn/machine-learning/ungradedLab/7GEJh/optional-lab-multiple-linear-regression/lab)
- [Local](/code/C1_W2_Lab02_Multiple_Variable_Soln.ipynb)

## Quiz: Multiple linear regression

1. In the training set below, what is$x_4^{(3)} $?

| Size | Rooms | Floors | Age | Price |
| -    | -     | -      | -   | -     |
| 2104 | 5     | 1      | 45  | 460   |
| 1416 | 3     | 2      | 40  | 232   |
| 1534 | 3     | 2      | 30  | 315   |
| 852  | 2     | 1      | 36  | 178   |

2. Which of the following are potential benefits of vectorization?
- [ ] It makes your code run faster
- [ ] It makes your code shorter 
- [ ] It allows your code to run more easily on parallel compute hardware
- [ ] All of the above

3. To make a gradient descent converge about twice as fast, a technique that almost always works is to double the learning rate$alpha $
- [ ] True
- [ ] False 

<details><summary>Ans</summary>30, 4, F</details>

# C1_W2_M2 Gradient Descent in Practice

## C1_W2_M2_01 Feature scaling part 1

![](/img/1.2.2.01.values.png)
- Use __Feature Scaling__ to enable gradient descent to run faster

![](/img/1.2.2.01.size.png)
- when we scatterplot size vs bedrooms, we see `x` has a much larger range than `y`
- when we _contour plot_ we see an oval
- ie small `w(size)` has a large change & _large_ `w(bedrooms` has a small change

![](/img/1.2.2.01.scale.png)
- since contour is tall & skinny, gradeient descent may end up bouncing back and forth for a long time
- a technique is to _scale the data_ to get a more _circular contour plot_

:bulb: We can __speed up gradient descent by scaling our features__

## C1_W2_M2_02 Feature scaling part 2

![](/img/1.2.2.02.scale.png)
- scale by dividing$x_i^{(j)} / \max_x $

![](/img/1.2.2.02.mean.normalization.png)
- __Mean Normalization__

![](/img/1.2.2.02.z.score.normalization.png)
- __Z-score Normalization__ also called __Gaussian Distribution__

![](/img/1.2.2.02.feature.scaling.png)
- When __Feature Scaling__ we want to range somewhere between `-1 <==> 1`
- but the range is ok if it's relatively close
- rescale if range is too large or too small

### Quiz:

Which of the following is a valid step used during feature scaling? (see bedrooms vs size scatterplot)
- [ ] Multiply each value by the maximum value for that feature
- [ ] Divide each value by the maximum value for that feature

<details><summary>Ans</summary>2</details>

## C1_W2_M2_03 Checking gradient descent for convergence

![](/img/1.2.2.03.alpha.png)
- We can choose$\alpha $

![](/img/)
- Want to minimize _cost function_ $\min\limits_{\vec{w}, b} J(\vec{w}, b)$

## C1_W2_M2_04 Choosing the learning rate
## C1_W2_M2_05 Optional Lab: Feature scaling and learning rate
## C1_W2_M2_06 Feature engineering
## C1_W2_M2_07 Polynomial regression
## C1_W2_M2_08 Optional lab: Feature engineering and Polynomial regression
## C1_W2_M2_09 Optional lab: Linear regression with scikit-learn
## C1_W2_M2_10 Practice quiz: Gradient descent in practice
## C1_W2_M2_11 Week 2 practice lab: Linear regression
![](Screenshot%202024-10-09%20180220.png)
![](Screenshot%202024-10-09%20180317%201.png)